# 1D-CNNé¢„æµ‹æ¨¡å‹
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import r2_score, mean_squared_error
plt.rcParams['font.family'] = 'Arial'  # è®¾ç½®ä¸ºé»‘ä½“ï¼Œé€‚åˆä¸­æ–‡æ˜¾ç¤º


# è‡ªå®šä¹‰MAPEè®¡ç®—å‡½æ•°
def mean_absolute_percentage_error(y_true, y_pred):
    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100

# åŠ è½½æ•°æ®
data = pd.read_csv('loads.csv')  # æ›¿æ¢ä¸ºå®é™…æ–‡ä»¶å

# åˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡å€¼
X = data.iloc[:, 1:13].values  # å–Load-1åˆ°Load-12
y = data['WorstLife'].values   # å–WorstLife

# æ•°æ®é¢„å¤„ç†
# 1. å¯¹ç›®æ ‡å€¼å–å¯¹æ•°ï¼Œç¼“è§£åæ€åˆ†å¸ƒ
y_log = np.log(y + 1)  # åŠ 1é¿å…log(0)

# 2. å½’ä¸€åŒ–
scaler_X = MinMaxScaler()
scaler_y = MinMaxScaler()
X_scaled = scaler_X.fit_transform(X)
y_scaled = scaler_y.fit_transform(y_log.reshape(-1, 1))

# è½¬æ¢ä¸ºPyTorchå¼ é‡
X_tensor = torch.tensor(X_scaled, dtype=torch.float32)
y_tensor = torch.tensor(y_scaled, dtype=torch.float32)

# å°†ç‰¹å¾é‡æ–°ç»„ç»‡ä¸ºé€‚åˆ 1D CNN çš„å½¢çŠ¶ (æ ·æœ¬æ•°, é€šé“æ•°, ç‰¹å¾é•¿åº¦)
X_tensor = X_tensor.unsqueeze(1)  # æ·»åŠ é€šé“ç»´åº¦

# æ•°æ®åˆ’åˆ†
X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.1, random_state=42)

class CNNTransformerWithPE(nn.Module):
    def __init__(self, seq_len=12, d_model=64):
        super(CNNTransformerWithPE, self).__init__()
        self.conv1 = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv1d(in_channels=32, out_channels=d_model, kernel_size=3, padding=1)  # è¾“å‡ºç»´åº¦ä¸º d_model

        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.1)

        # Learnable Position Encodingï¼ˆ[1, seq_len, d_model]ï¼‰
        self.position_encoding = nn.Parameter(torch.randn(1, seq_len, d_model))

        # Transformer Encoder
        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=4, dim_feedforward=128, batch_first=True)
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)

        # è¾“å‡ºå±‚
        self.fc1 = nn.Linear(d_model * seq_len, 128)
        self.fc2 = nn.Linear(128, 1)

    def forward(self, x):
        x = self.relu(self.conv1(x))     # [B, 32, 12]
        x = self.relu(self.conv2(x))     # [B, d_model, 12]

        x = x.permute(0, 2, 1)           # [B, 12, d_model]
        x = x + self.position_encoding   # åŠ ä¸Šä½ç½®ç¼–ç 

        x = self.transformer_encoder(x)  # [B, 12, d_model]
        x = x.reshape(x.size(0), -1)     # å±•å¹³

        x = self.dropout(self.relu(self.fc1(x)))
        x = self.fc2(x)
        return x

# å®ä¾‹åŒ–æ¨¡å‹
model = CNNTransformerWithPE()

# å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
criterion = nn.MSELoss()  # ä½¿ç”¨ MSE æŸå¤±
optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam ä¼˜åŒ–å™¨
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=15)  # å­¦ä¹ ç‡è¡°å‡

# è®¾ç½®è®­ç»ƒå‚æ•°
epochs = 200
batch_size = 32

# åˆ›å»ºæ•°æ®é›†å’Œ DataLoader
train_dataset = torch.utils.data.TensorDataset(X_train, y_train)
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

# è®­ç»ƒå¾ªç¯
losses = []
val_losses = []
best_val_loss = float('inf')
patience = 30
counter = 0

for epoch in range(epochs):
    model.train()
    epoch_loss = 0.0
    for batch_X, batch_y in train_loader:
        # å‰å‘ä¼ æ’­
        predictions = model(batch_X)
        loss = criterion(predictions, batch_y)

        # åå‘ä¼ æ’­å’Œä¼˜åŒ–
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        epoch_loss += loss.item()
    epoch_loss /= len(train_loader)
    losses.append(epoch_loss)

    # éªŒè¯é›†æ€§èƒ½
    model.eval()
    with torch.no_grad():
        val_predictions = model(X_test)
        val_loss = criterion(val_predictions, y_test)
        val_losses.append(val_loss.item())

    # å­¦ä¹ ç‡è¡°å‡
    scheduler.step(val_loss)

    # æ—©åœæœºåˆ¶
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        counter = 0
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        torch.save(model.state_dict(), 'best_1dcnn_model.pth')
    else:
        counter += 1
        if counter >= patience:
            print(f"Early stopping at epoch {epoch + 1}")
            break

    print(f"Epoch {epoch + 1}/{epochs}, Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}")

# åŠ è½½æœ€ä½³æ¨¡å‹
model.load_state_dict(torch.load('best_1dcnn_model.pth', weights_only=True))

# è¯„ä¼°æ¨¡å‹
model.eval()
with torch.no_grad():
    y_pred = model(X_test)
    test_loss = criterion(y_pred, y_test)
    print(f"Test Loss: {test_loss.item():.4f}")

    # åå½’ä¸€åŒ–
    y_pred_rescaled = np.exp(scaler_y.inverse_transform(y_pred.numpy())) - 1
    y_test_rescaled = np.exp(scaler_y.inverse_transform(y_test.numpy())) - 1

    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    r2 = r2_score(y_test_rescaled, y_pred_rescaled)
    mape = mean_absolute_percentage_error(y_test_rescaled, y_pred_rescaled)
    rmse = np.sqrt(mean_squared_error(y_test_rescaled, y_pred_rescaled))
    print(f"RÂ²: {r2:.4f}, MAPE: {mape:.4f}%, RMSE: {rmse:.2f}")

    # è¾“å‡ºéƒ¨åˆ†é¢„æµ‹å€¼
    print("Predicted WorstLife:", y_pred_rescaled[:20].flatten())
    print("True WorstLife:", y_test_rescaled[:20].flatten())

# æ¦‚ç‡é¢„æµ‹éƒ¨åˆ† - å¤šæ¬¡é¢„æµ‹
num_samples = 100  # é¢„æµ‹æ¬¡æ•°
all_predictions = []

# å¯ç”¨dropoutä»¥è·å–ä¸ç¡®å®šæ€§ä¼°è®¡
model.train()  # æ³¨æ„è¿™é‡Œä½¿ç”¨trainæ¨¡å¼ä»¥å¯ç”¨dropout

with torch.no_grad():
    for _ in range(num_samples):
        # æ¯æ¬¡é¢„æµ‹éƒ½ä¼šå› dropoutè€Œç•¥æœ‰ä¸åŒ
        pred = model(X_test)
        pred_rescaled = np.exp(scaler_y.inverse_transform(pred.numpy())) - 1
        all_predictions.append(pred_rescaled.flatten())

all_predictions = np.array(all_predictions)  # shape: (num_samples, num_test_points)
y_test_rescaled = y_test_rescaled.flatten()

# è®¡ç®—ç»Ÿè®¡é‡
mean_predictions = np.mean(all_predictions, axis=0)
std_predictions = np.std(all_predictions, axis=0)

# æ‰¾åˆ°MAPEæœ€å°çš„ä¸€ç»„é¢„æµ‹
mape_values = [mean_absolute_percentage_error(y_test_rescaled, pred) for pred in all_predictions]
best_idx = np.argmin(mape_values)
best_prediction = all_predictions[best_idx]

# ç»˜åˆ¶æ¦‚ç‡é¢„æµ‹å›¾
plt.figure(figsize=(5.8, 6))

# ç»˜åˆ¶çœŸå®å€¼å’Œé¢„æµ‹å‡å€¼
#plt.scatter(y_test_rescaled, mean_predictions, s=100, alpha=0.7, label='Mean prediction')

# ç»˜åˆ¶æœ€ä½³MAPEé¢„æµ‹ç‚¹
plt.scatter(y_test_rescaled, best_prediction, s=10, alpha=1, color='#FD6E6F', label='Test data',zorder=0.6)

# ç»˜åˆ¶y=xå‚è€ƒçº¿
x_line = np.linspace(0.8*min(y_test_rescaled), 1.2*max(y_test_rescaled), 100)
plt.plot(x_line, x_line, 'r--', label='y = x', linewidth=2,zorder=0.5)


# ç»˜åˆ¶ç½®ä¿¡åŒºé—´
sorted_idx = np.argsort(y_test_rescaled)
y_test_sorted = y_test_rescaled[sorted_idx]
mean_sorted = mean_predictions[sorted_idx]
std_sorted = std_predictions[sorted_idx]

# ç»˜åˆ¶å‡å€¼çº¿
plt.plot(y_test_sorted, mean_sorted, color='#D4D4D4', linestyle='-', linewidth=2, label='Mean (Î¼)',zorder=0.3)

# Î¼ Â± Ïƒ
plt.fill_between(y_test_sorted,
                 mean_sorted - std_sorted,
                 mean_sorted + std_sorted,
                 color='#001080', alpha=1, label='Î¼ Â± Ïƒ',zorder=0.2)

# Î¼ Â± 2Ïƒ
plt.fill_between(y_test_sorted,
                 mean_sorted - 2*std_sorted,
                 mean_sorted + 2*std_sorted,
                 color='#50BDFF', alpha=1, label='Î¼ Â± 2Ïƒ',zorder=0.1)

# Î¼ Â± 3Ïƒ
plt.fill_between(y_test_sorted,
                 mean_sorted - 3*std_sorted,
                 mean_sorted + 3*std_sorted,
                 color='#B7E5FF', alpha=1, label='Î¼ Â± 3Ïƒ',zorder=0)


# è®¾ç½®å¯¹æ•°åæ ‡
plt.xscale('log')  # xè½´å¯¹æ•°åæ ‡
plt.yscale('log')  # yè½´å¯¹æ•°åæ ‡

# è®¾ç½®ä¸»åˆ»åº¦å’Œæ¬¡åˆ»åº¦çš„é•¿åº¦
plt.tick_params(axis='both',  # åº”ç”¨åˆ°æ‰€æœ‰åæ ‡è½´
                which='major',  # è®¾ç½®ä¸»åˆ»åº¦
                length=5,      # ä¸»åˆ»åº¦é•¿åº¦
                width=1.5,        # ä¸»åˆ»åº¦ç²—ç»†
                labelsize=18)

plt.tick_params(axis='both',  # åº”ç”¨åˆ°æ‰€æœ‰åæ ‡è½´
                which='minor',  # è®¾ç½®æ¬¡åˆ»åº¦
                length=2.5,       # æ¬¡åˆ»åº¦é•¿åº¦
                width=1.5)        # æ¬¡åˆ»åº¦ç²—ç»†

# è®¾ç½®åæ ‡è½´çš„ç²—ç»†
for spine in plt.gca().spines.values():
    spine.set_linewidth(1.5)

plt.xlabel("Tested (Repeats)", fontsize=20)
plt.ylabel("Predicted (Repeats)", fontsize=20)
plt.title("3 Sensors", fontsize=20)
plt.xlim([0.6*min(y_test_rescaled),1.4*max(y_test_rescaled)])
plt.ylim([0.6*min(y_test_rescaled),1.4*max(y_test_rescaled)])
# æ ‡æ³¨è¯„ä¼°æŒ‡æ ‡
text = f"RÂ² = {r2:.4f}\nMAPE = {mape:.2f}%"
ax = plt.gca()
plt.text(0.05, 0.9, text, transform=ax.transAxes, fontsize=18,verticalalignment="center", horizontalalignment="left")

# æ˜¾ç¤ºå›¾åƒ
plt.legend(fontsize=18, loc='lower right', edgecolor='black', fancybox=False, shadow=True)
plt.tight_layout()
plt.savefig('1DCNN_Transformer_Uncertainty.png', dpi=600, bbox_inches='tight')
plt.show()

# ==========================
# SHAP åˆ†æä¸å¯è§†åŒ–
# ==========================
import shap
import numpy as np
import matplotlib.pyplot as plt

# é€‰å–æ ·æœ¬æ•°
sample_X = X_scaled[:150]

# å®šä¹‰SHAPä½¿ç”¨çš„æ¨¡å‹é¢„æµ‹å‡½æ•°
def shap_predict(input_array):
    input_tensor = torch.tensor(input_array, dtype=torch.float32).unsqueeze(1)
    with torch.no_grad():
        output = model(input_tensor).numpy()
    return output

# è®¾ç½®èƒŒæ™¯æ•°æ®ï¼ˆ20ä¸ªæ ·æœ¬ï¼‰
background = X_scaled[np.random.choice(X_scaled.shape[0], 20, replace=False)]

# åˆ›å»ºè§£é‡Šå™¨å¹¶è®¡ç®— SHAP å€¼
explainer = shap.KernelExplainer(shap_predict, background)
shap_values = explainer.shap_values(sample_X)
print("type(shap_values):", type(shap_values))
print("shap_values shape:", shap_values.shape)
print("sample_X shape:", sample_X.shape)

# ç‰¹å¾å
feature_names = np.array([f"Load_{i+1}" for i in range(sample_X.shape[1])])

# å»é™¤ SHAP æœ€åä¸€ç»´ï¼Œä¿è¯æ˜¯ (50, 12)
shap_values = shap_values.squeeze(-1)

# æ£€æŸ¥å½¢çŠ¶æ˜¯å¦åŒ¹é…
assert shap_values.shape == sample_X.shape, "Shape mismatch after squeezing."

# ============================
# ğŸ¯ å·¦ç§»ç‰¹å¾é¡ºåº
# ============================
shap_values = np.roll(shap_values, shift=-1, axis=1)
#feature_names = np.roll(feature_names, shift=-1)

# ============================
# ğŸ¯ å¹³å‡ SHAP æŸ±çŠ¶å›¾
# ============================
mean_abs_shap = np.abs(shap_values).mean(axis=0)
mean_abs_shap = np.array(mean_abs_shap).astype(float).flatten()

plt.figure(figsize=(8.5, 7))
bars = plt.bar(feature_names, mean_abs_shap, color='blue')
plt.ylabel("Mean SHAP value", fontsize=20)

# æ·»åŠ æ•°å€¼æ ‡ç­¾ï¼ˆä¿ç•™3ä½å°æ•°ï¼‰
for bar in bars:
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, height + 0.001,
             f"{height:.3f}", ha='center', va='bottom', fontsize=16)

for spine in plt.gca().spines.values():
    spine.set_linewidth(1.5)  # è®¾ç½®åæ ‡è½´çº¿çš„ç²—ç»†

plt.tick_params(axis='both',  # åº”ç”¨åˆ°æ‰€æœ‰åæ ‡è½´
                width=1.5,        # ä¸»åˆ»åº¦ç²—ç»†
                labelsize=16)
plt.ylim([0,0.085])
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig("SHAP_Bar.png", dpi=600)
plt.show()

# ============================
# ğŸ¯ SHAP åˆ†å¸ƒå›¾
# ============================
plt.figure(figsize=(8, 6))
shap.summary_plot(shap_values, sample_X, feature_names=feature_names, show=False)
plt.savefig("SHAP_Summary.png", bbox_inches="tight", dpi=600)
plt.show()

# ==========================
# æ°”æ³¡çƒ­åŠ›å›¾
# ==========================
import pandas as pd
feature_names = [f"L{i+1}" for i in range(12)]
df_features = pd.DataFrame(X_scaled, columns=feature_names)

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors

# æ„å»ºç‰¹å¾å
corr_matrix = df_features.corr(method='pearson').values

# è‡ªå®šä¹‰ SHAP é£æ ¼é…è‰²
cmap = mcolors.LinearSegmentedColormap.from_list(
    "custom_shap",
    [(0, (0/255, 138/255, 251/255)),   # è“è‰²
     (0.5, (128/255, 0/255, 128/255)), # ç´«è‰²
     (1, (255/255, 0/255, 82/255))]    # çº¢è‰²
)

fig, ax = plt.subplots(figsize=(8, 7))

# ç»˜åˆ¶æ°”æ³¡
for i in range(len(feature_names)):
    for j in range(len(feature_names)):
        value = corr_matrix[i, j]
        size = abs(value) * 500  # è°ƒæ•´æ°”æ³¡å¤§å°
        color = cmap((value+1)/2)
        ax.scatter(j, len(feature_names) - 1 - i, s=size, c=[color], alpha=0.9, edgecolors='k')

# è®¾ç½®åæ ‡è½´
ax.set_xticks(range(len(feature_names)))
ax.set_yticks(range(len(feature_names)))
ax.set_xticklabels(feature_names, fontsize=16)
ax.set_yticklabels(feature_names[::-1], fontsize=16)

# âœ… ç”»æ ¼å­æ¡†ï¼Œè€Œä¸æ˜¯ç½‘æ ¼çº¿ç©¿è¿‡æ°”æ³¡
ax.set_xticks(np.arange(-0.5, len(feature_names), 1), minor=True)
ax.set_yticks(np.arange(-0.5, len(feature_names), 1), minor=True)
ax.grid(which="minor", color="black", linestyle="-", linewidth=1.0)
ax.tick_params(which="minor", bottom=False, left=False)
ax.set_xlim(-0.5, len(feature_names)-0.5)
ax.set_ylim(-0.5, len(feature_names)-0.5)

# æ·»åŠ é¢œè‰²æ¡
norm = mcolors.Normalize(vmin=-1, vmax=1)
sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)
cbar = plt.colorbar(sm, ax=ax)
cbar.ax.tick_params(labelsize=14)
cbar.set_label("Pearson Correlation", fontsize=16)

ax.set_title("Bubble heat map of bolt preload correlations", fontsize=16)
plt.tight_layout()
plt.savefig("Bubble_Correlation_Boxed.png", dpi=600)
plt.show()

